---
title: "Literature Review"
author: "Chelsea Matthews"
date: "27 August 2019"
output: html_document
bibliography: bibliography.bib
---

## Aim
To find evidence and provide guidance on the selection of sequencing technologies and assembly methods to provide adequate assembly quality within constrained resources. 
This will be achieved through a benchmarking study of de novo assembly pipelines using gold standard assembly tools and a range of sequencing platforms for the rice strain Minghui 63. This should include coverage requirements (and inferred associated sequencing costs), CPU hours for an assembly and a quality assessment of the resulting genome with a focus on the optimisation of available resources for a given project to meet time, cost and quality requirements where possible.   
Deliverables will include a range of reproducible pipelines for de-novo genome assembly as well as for benchmarking assessment. 


## Introduction

There are two primary methods of genome assembly; a reference-guided assembly where reads are compared with a reference genome of the same species or a closely related organism and a de novo assembly where overlapping reads are assembled from scratch using an assembly algorithm [@kyrikidou_2018]. Reference-guided assembly (often termed resequencing when a reference genome of the same species is used) is computationally cheap compared with de novo assemblies which essentially require an 'all-to-all' comparison of reads. However, reference-guided assemblies are often plagued by reference bias, where variation not present in the reference genome is missed in the genome being assembled. De novo assembly requires a greater depth of genome coverage but solves the reference bias problem [@baker_2012; @chin_2019]. 

In bacteria, de novo assembly is fairly routine as the small genome size results in a low complexity of assembly which, in turn, results in a cheap assembly. As genome size increases, de novo assembly becomes much more expensive due to the increased complexity of the assembly and hence, the amount of work and time required. In a complex de novo assembly, the cost of the sequencing itself becomes insignificant (per kb) when compared with the cost of the bioinformaticians time. 

While a high quality assembly is obviously preferred, it is not always possible or even necessary, depending on the biological question being investigated. Resource availability including money, knowledge and time, heavily influence what assembly quality can reasonably be achieved. There is a huge amount of research available regarding the differences between assembly tools and underlying algorithms [@cherukuri_2016; @giordano_2017; @khan_2018], the quality of genomes assembled using data from different sequencing platforms [@wenger_2019; @giordano_2017] and the error profiles of sequencing technologies [list of references here]. However, there is a distinct lack of information available to help a researcher maximise the quality of an assembly given their available resources so that their biological question can be fully addressed. In filling this gap, we review the current literature including current sequencing technologies, methods of measuring assembly quality, an investigation into different workflows and methods for assembly as well as ways to ensure workflow reproducibility. This information will then be used to design a benchmarking comparison of different methods for assembly including an analysis of sequencing costs, CPU hours, and the resulting assembly quality. 

## The first human assembly

The first human genome assembly was a mammoth task and began in 1990 with a budget of \$3 billion US [@HGP_2003]. The project completed two years ahead of schedule (and under budget) in 2003 and resulted in an assembly that covered approximately 99% of the gene-containing regions of the genome and was 99.99% accurate [@HGP_2003]. This project was made possible by Sanger, Nickeln and Coulson when they published their gamechanging paper "DNA sequencing with chain-terminating inhibitors" in 1977. In this paper, they introduced what is now termed Sanger sequencing, a method of sequencing DNA that is relatively robust, easy to use and accurate and was the method of choice for the Human Genome Project. Sanger sequencing uses DNA Polymerase to synthesise the complementary strand of a single-stranded DNA molecule in the presence of a small number of chain-terminating dideoxynucleotides which results in a library of double stranded fragments terminating at various different locations. Through fractioning these fragments with electrophoresis, the resulting bands give the sequence of nucleotides (See Figure 1 below for further detail on this process) [@sanger_1977; @frana_2002].

<br>
<center>

![Figure 1: The Sanger Sequencing technique. Source: @gauthier_2007](images/sanger_method.png){width=70%}

</center>
<br>

Sanger sequencing yields highly accurate reads (99.99% accurate) of approximately 600bp in length (up to a maximum of ~1000bp [@]) and errors tend to be primarily substitutions [@lutteropp_2017]. However, despite advances in sanger sequencing technology resulting in improved throughput and reduced cost, it was, and still is, prohibitively expensive (~\$5 US for between 1 and 10 samples down to ~\$3 US for between 97 and 480 samples (taken from @biobasic_price)) and slow. Enter high throughput sequencing. 

## High Throughput Sequencing

The next generation of sequencing technology began to emerge around the same time that large-scale chain-termination methods were being developed [@heather_2016] and was characterised by the ability to sequence millions of DNA fragments at the same time [@behjati_2013]. High throughput sequencing techniques can be subdivided into at least two sub-categories; second generation techniques and single molecule sequencing. 

#### Second Generation

Illumina is by far the most commonly used second generation technology [@greenleaf_2014]. It provides a range of options for both benchtop and production scale sequencing and is capable of producing both single and paired end reads of various lengths (up to 250bp). Illumina uses a patented sequencing by synthesis technology. First, library preparation is completed by randomly fragmenting DNA and ligating adapters to both ends of each fragment. These fragments are then PCR amplified and purified to form the library. This library is then loaded onto a flow cell where each fragment is amplified into a cluster so that sequencing can begin. The sequencing process itself consists of imaging fluorescently tagged reversible terminators as they are incorporated and then claved from the fragment clusters by DNA polymerase [@buermans_2014]. See Figure 2 below for more detail on cluster generation and sequencing. 

<br>
<center>

![Figure 2: Illumina sequencing method.  Source: @illumina_image](images/illumina_method.png){width=100%}

</center>
<br>

While this method results in high quality reads (~99.8 accuracy in forward reads and ~99.6 for reverse reads), the error distribution is not random with a strong bias towards substitution errors following the motifs 'CGG' and 'GGG' [@schirmer_2016]. Substitutions are the main error type in illumina reads (1000 times more likely than insertions or deletions) with errors tending to accumulate near the end of the read due to phasing issues during the run obscuring the true signal from a cluster. Many authors also report limitations due to preferential amplification [@aird_2011; @krehenwinkel_2017] though @pfeiffer_2018 found no evidence of bias in their study. Illumina reads are also very cheap with a cost of between 2.4 and 2.9c/Mbp (based on a recent quote).

* comment on GC bias for illumina
* more about bias due to amplification.
* show how error rates are connected to sequencing method

#### Single Molecule Sequencing

Single Molecule Sequencing techniques negate the need for amplification of DNA fragments and hence, remove the associated errors and biases [@schadt_2010]. PacBio provided the first widely used single molecule sequencing technology with the SMRT (Single-Molecule, Real-Time) sequencing technique. In this technique, the real-time addition of bases is observed by immobilising a DNA polymerase in a Zero Mode Waveguide illuminated from below by a laser. This enables the detection of the incorporation of individual phospholinked nucleotides against the bulk solution background (see Figure 1 below for more detail) [@eid_2009]. PacBio sequencing also makes use of the SMRTbell template format where single stranded hairpin adapters are ligated to both ends of the double stranded fragment of DNA. Then, during sequencing using the SMRT platform, the DNA polymerase begins incorporating bases, starting at the primer, around the circular fragment. If it travels the full circle and reaches the location where it began, the DNA polymerase will displace the new strand and continue sequencing. Using this method, both the sense and anti-sense strands can be sequenced resulting in either very long reads of low quality (~15% error rate from a single pass raw read [@wei_2018]), or shorter more accurate reads (consensus sequences) can be generated from multiple passes of the molecule [@travers_2010] (see Figure 2). Unlike first and second generation technologies, PacBio errors are randomly distributed [@eid_2009] which means that consensus sequences greatly reduce basecalling error [@travers_2010] and that, in the case of very long fragments being sequenced, increasing coverage enables a consensus across reads to be reached (a method commonly used for polishing assemblies) [@chin_2013]. 

PacBio Hifi reads, an optimisation of circular consensus sequencing methods, [@wenger_2019] boast sequences that are both long and highly accurate. By selecting for fragments of approximately 15kb in length (using SageELF for separation into 3kb fractions and Agilent 2100 BioAnalyzer to identify fractions centered around 15kb), Sequel produced an average consensus sequence length of 13.5±1.2 kb with mean read concordance (when compared with the reference) at 99.8% (compared with 99.9% concordance for illumina reads). The recently released Sequel II boasts even greater accuracy than Sequel as well as higher throughput [@sequel2_blog]. This demonstrates the utility of PacBio Hifi reads with sequence lengths much greater than Sanger or Illumina reads as well as comparable accuracy.  

* more about error profile and cost/throughput on sequel 2
* show how error rates are connected to sequencing method
* 

[@deamer_2016]

<br>
<center>

![Figure 1: Principle of SMRT DNA sequencing. Source: [@eid_2009]](images/pacbioSMRT.png){width=50%}


![Figure 2: CCS generation. Source: [@pacificbiosciences_2019]](images/ccs-workflow.png){width=50%}

</center>
<br>

**Fix the pacbio github reference above. not sure how to reference without a doi**

Apart from PacBio, Oxford Nanopore Technologies (ONT) is the other major provider of third generation sequencing technology. The MinION device was released in 2014 [@mikheyev_2014] and costs just $1000 US for a starter pack. It reports data in real time beginning just two minutes after the sequencing run begins (with 10 minutes sample preparation time using the Rapid Kit) and is literally pocket sized **(Reference Nanopore website here)**. Nanopore technology directly sequences a single strand of DNA by threading it through a nanopore using a molecular motor protein. As the molecule moves through the pore, with approximately 5 or 6 bases occupying the pore at one time, changes in current across the pore are measured and a basecaller is used to determine the nucleotide sequence based on these changes. Using this method, a 1D read can be obtained by sequencing the fragment in a single pass. To improve data quality, a technique to generate a 1D^2 read has been developed where both the sense and anti-sense strands are sequenced sequentially to generate a consensus sequence [@patel_2018]. @weirather_2017 estimated the raw error rate of 1D reads to be ~20% while @volden_2018 found a mean error rate of 13% for 1D reads. The 1D^2^ method improves read accuracy and can reduce the error rate to ~5% [@volden_2018] but, in comparison to PacBio Hifi reads (at 99.8% accuracy), this is still quite low. Due to the design of the nanopore platform, circularising DNA fragments, in a similar manner to PacBio Hifi reads, is not possible but there is work investigating the use of Rolling Circle Amplification to create long strands of cDNA with multiple copies of the fragment of interest before sequencing on nanopore platforms to emulate PacBio Hifi reads [@volden_2018] *(see Figure ?)*. 

The error distribution in nanopore data is not entirely random with deletion errors in sequences overlapping homopolymers being reported as up to 2.6 times more likely [@stancu_2017]. In general, indels were the most common type of error with deletions accounting for approximately 9% of the total 15% while mismatches were the next most common type of error at 5.1% [@stancu_2017]. There are two main sources of error in nanopore reads; errors which occur during sequencing and are inherent in the raw data and errors made by the basecalling algorithm [@rang_2018]. In the latter case, the necessary information is present in the current but is called incorrectly by the basecaller [@rang_2018]. Of these, the second type can often be reduced by training a basecalling algorithm on a closely related species [@wick_2019, @jain_2018]. The initial outlay for a MinION starter kit is just \$1000 US with additional sequencing kits costing approximately $600 US, depending on the type of kit. As an example, the 1D^2^ sequencing kit typically yields 8+ Gb in 48 hours which, assuming 8Gb equates to 7.5c/kb [@ont_store]. 

## Quality metrics

There are a massive number of projects currently being undertaken involving the de novo assembly of previously unsequenced organisms. Some of these include the Global Invertebrates Genomics Alliance (GIGA) [@gigacommunityofscientists_2014], the Genome 10K Project [@koepfli_2015] and, most recently, the Earth BioGenome Project with the lofty goal of sequencing 'all of Earth's eukaryotic biodiversity over a period of ten years' [@lewin_2018] which includes a de novo assembly of at least one member from each family.

With such a large number of sequencing projects underway [@gigacommunityofscientists_2014; @lewin_2018; @koepfli_2015], it is essential that we are able to accurately measure the quality and completeness of a de novo assembly [@yang_2019]. Probably the most commonly cited metric of assembly quality is contig N50 [@kyriakidou_2018] where a contig N50 of 1Mbp implies that contigs of 1Mbp and greater comprise 50% of the total assembly size. However, this only gives the reader an indication of the distribution of the contigs, can easily be manipulated through more or less stringent filtering of short contigs [@molecularecologist_n50] and can be artificially increased by specifying less stringent requirements for the concatenation of contigs [@gurevich_2013]. For these reasons, contig N50 alone isn't enough to measure assembly quality. QUAST, Quality Assessment Tool for genome assemblies released, improves on this by reporting contig Nx in addition to the total number of contigs, length of the largest contig, total length, GC content and the number of predicted genes in a de novo assembly [@gurevich_2013]. While this does give a better indication of the data distribution, it still doesn't give any information regarding base quality or the likely completeness of the assembly. 

Another approach to assessing assembly completeness is demonstrated by BUSCO (Benchmarking Universal Single-Copy Orthologs) which benchmarks the ortholog content of a new assembly against a curated ortholog set generated from a selection of organisms belonging to the same clade [@simo_2015; @waterhouse_2017]. BUSCO then reports the number of complete orthologs found with only a single copy, complete orthologs with multiple copies, fragmented orthologs and missing orthologs [@simo_2015]. The higher the number of complete single copy orthologs, the more complete the assembly is likely to be while high numbers of multiple copy orthologs are likely to indicate that the assembly of haplotypes is incorrect [@simo_2015]. Another method for assessing the completeness of an assembly (primarily in plants) is the LTR Assembly Index (LAI) [@ou_2018_lai]. Given that LRT Retrotransposons comprise a large portion of most plant genomes [@ou_2017] and that they are notoriously difficult to sequence due to their repetitive nature, a measure of assembly continuity of these regions is warranted [@ou_2018_lai]. Ou, Chen and Jiang noted that more complete genomes had a higher proportion of intract LTR elements when compared with less complete genomes (2018).To leverage this fact, they used LTR_retriever [@ou_2018_retriever] to first identify all LTR elements, then identified all intact LTR elements and finally applied a transformation to estimate LAI (see Equation 1 below) where LAI values less than 10 indicate draft genomes, between 10 and 20 are reference quality and LAI scores over 20 indicate gold standard assemblies [@ou_2018_lai]. 
As many new assemblies obtain very high BUSCO scores [@ou_2018_lai], this method helps to further differentiate between a good and a great assembly by evaluating the quality of one of the more difficult to assemble region of the genome, if only in plants. 


<br>
Equation 1:

$$LAI = \frac{Intact \space LTR \space element \space length}{Total \space LTR \space sequence \space length}\times 100 + 2.8138 \times (94 - Whole \space genome \space LTR \space identity)$$
<br>

It is also worth investigating the methods proposed for assessing assembly quality in some of the major sequencing projects currently underway. The Earth BioGenome Project proposes to use a combination of contig N50, scaffold N50, a measure of chromosome assignment and a measure of basecall quality while avoiding assigning a direct classification (eg. gold, platinum, draft) [@lewin_2018]. GIGA proposes a similar approach but will also report, where possible, percent gaps, percent detection of conserved eukaryotic genes, a statistical assessment of assembly, alignment to any available syntenic or physical maps and mapping statistics of any availabla transcript data where possible [@gigacommunityofscientists_2014]. It appears as though both of these projects will be staying away from specific labelling of an assembly and will report only quantifiable measurements so that, as sequencing and assembly technology improves, these labels won't become redundant. 

## De Novo Assembly Tools and Workflows

There are countless different paths to generating a de novo assembly but all of them begin with at least one type of sequencing data. In the beginning, the human genome was sequenced using primarily Sanger sequencing techniques [@lander_2001] but the advent of Next Generation techniques and the associated high throughput has made sequencing on a much larger scale possible [@pollard_2018]. Sequencing by synthesis methods meant that a entire human genome could be sequenced in a day compared with years for the first human genome draft using Sanger techniques [@behjati_2013]. However, Illumina reads (single or paired end), while very accurate, simply aren't long enough to reconstruct some of the more complex and repetitive regions of the genome [@pollard_2018; @sovi_2016], are not suited for discovery of structural variants [@cretustancu_2017]] and are further limited by preferential amplification of certain fragments. To overcome these shortfalls, third generation sequencing techniques have been invaluable, providing long reads to give context in extended repetitive regions, enabling the discovery of structural variants as well as removing bias associated with amplification of DNA fragments. A large amount of research has made it very clear that the incorporation of at least some long reads into a de novo assembly greatly improves the final assembly quality [@shin_2019; @jain_2018; @rhoads_2015]. 

The de novo assembly of a genome using long reads can be done in a number of ways and the complexity of these workflows is somewhat dependent on the platform(s) selected for sequencing. For example, a simple workflow for the assembly of a genome using PacBio Hifi reads would require the generation of consensus reads (ccs software) and the assembly of these reads into the final genome (peregrine) [@chin_2019]. In comparison, a hybrid assembly incorporating both nanopore and Illumina reads could require assembly of the nanopore reads (Canu), polishing contigs (racon), mapping the Illumina dataset to contigs (BWA mem) and error correction of contigs using the Illumina reads (pilon) [@nanoporetech_2016]. It should also be noted that hybrid assemblies can be completed in number of different ways, not limited to the following:  

- Short reads are used for error correction of long reads prior to assembly [@goodwin_2015]
- Long, low quality reads used as scaffolds for the alignment of short reads [@ye_2016]
- Short reads used to polish errors from the long read assembly [@nanoporetech_2016]

It's interesting to note that the first method listed above was developed primarily to overcome the fact that existing assembly algorithms (Overlap-Layout-Consensus, generally used for Sanger data and the deBrujn graph, developed for Illumina reads) were unable to cope with long, low quality reads that were typical of third generation sequencing technology. Correcting errors in long reads at the beginning of a workflow made it possible to use assembly algorithms originally designed for sanger sequencing reads [@ye_2016], in particular those utilising the Overlap-Layout-Consensus algorithm. For assembly of long reads, the OLC algorithm appears to result in higher quality assemblies than the deBruijn graph [@cherukuri_2016] with higher coverage, larger average contig lengths and less contigs. 

There are many benchmarking studies comparing the performance of assembly and polishing algorithms on a particular dataset. Recently, @wenger_2019 compared de novo human genome assemblies from PacBio CCS reads using Canu, FALCON and wtdbg2 and found that all tools had high contiguity at 28x coverage (Canu N50 of 22.78Mb, FALCON N50 of 28.95Mb and wtdbg2 N50 of 15.43Mb) but that Canu and Falcon were over five times slower. However, Canu and FALCON were both able to span greater lengths of segmental duplications (63.6 and 46.1Mb respectivey) than wtdbg2 (26.4Mb) and Canu also managed to assemble some of the heterozygous alleles into separate contigs. They considered all three assemblies to be of high quality with wtdbg2 having the highest Phred concordance score of 44.6 with the reference (31.1 for Canu and 25.8 for FALCON). @giordano_2017 conducted a similar comparison of seven different long read assembly workflows as well as comparing the differences between MinION and PacBio assemblies (at 31x coverage). Tools compared were Canu, FALCON, PBcR-Self, ABruijn, SMARTdenovo, Miniasm and Racon. They found that Miniasm was the fastest to run on both PacBio and ONT but also the least accurate which is unsurprising as Miniasm has no consensus step [@li_2016] and hence, the final assembly has an error rate the same as the raw reads. For the ONT data, Racon (which polishes a miniasm assembly) resulted in the highest identity at 98.5% while Canu and PBcR-Self were the most accurate for the PacBio dataset with identities of 99.92 and 99.93 respectively. Overall, @giordano_2017 suggest that SMARTdenovo and Canu (both based on the Overlap-Layout-Consensus algorithm) were the best performers with 'longest reference coverage, best or near best average identity, highest number of genes found and long Na50s' for both the PacBio and nanopore datasets. Interestingly, SMARTdenovo was also the fastest of all workflows tested and took only 2 CPU hours while Canu took 15 hours for PacBio reads and 80 hours for ONT data. In comparison with these findings, @minei_2018 looked at the assembly quality of C. variabilis resulting from nanopore only assemblies (with 56x coverage of the ~46Mbp genome) using ABruijn, Miniasm and Canu and found that Canu resulted in just 26% coverage (2.5% identity) of the reference with ABruijn (the best performer) at 94.68% coverage (21% identity). They also reported that there was little improvement with coverage over 10-20X which is approximately in line with @giordano_2017 who found that an increase from 20X to 31X resulted in only slightly improved final accuracies. It is interesting that @giordano_2017 specifically noted Canu as being one of the best assemblers while @minei_2018 found it to be the worst. It is not clear what accounts for this although @minei_2018 suggest that Canu is simply not suitable for assembly of the *C. variabilis* genome using long reads. 

The workflows discussed so far are all in relation to a single source of input data but hybrid approaches are common and can be very useful for managing costs while still obtaining a high quality assembly **(get a ref if possible)**. In addition to the workflows above, @giordano_2017 also investigated a few hybrid workflows using long reads for scaffolding a short read assembly. They tested npScarf, HybridSPAdes and SMIS, all with a SPAdes MiSeq assembly and found that npScarf resulted in the least contigs (21 for ONT and 22 for PacBio) and in the least processing time but that these contigs were heavily impacted by misassemblies. HybridSPAdes produced assemblies with the greatest reference coverage, the least mis-assemblies and the most genes but at the cost of CPU hours (for ONT, HybridSPAdes was 6 times slower than npScarf and 1.3 times slower than SMIS, for PacBio HybridSPAdes was 13 times slower than npScarf and 3 times slower than SMIS) [@giordano_2017]. Hybrid workflows tested by @minei_2018 include SPAdes and MaSuRCA and they found that SPAdes resulted in an assembly with quality similar to a short read only assembly while MaSuRCA performed very well with the highest assembly quality and less contigs than the reference. 

**write more about different workflows and tools for assembly. Polishing section.**

## Reproducibility

Science depends on researchers being able to replicate the work of others but unfortunately, this isn't always possible. @baker found in a survey of 1500 scientists tht 70% of researchers had been unable to reproduce another scientist's experiment at some point while, even more worrying, 50% were unable to reproduce one of their own experiments. In bioinformatics, reproducibility refers specifically to being able to obtain the exact same results for an analysis using the original dataset and code but, even when the data and code are supplied, this isn't always possible [@kim_2018]. @kim_2018 suggest that the main barrier to reproducibility is the incompatibility among environments, software versions and different languages while @kulkarni_2018 attribute these issues to the short half-life of bioinformatics software, complexity of workflows, uncontrolled effects induced by changes to system libraries and the imprecision in workflow descriptions. To combat these difficulties, @kim_2018 recommend that researchers ensure their code is well written, well tested and made available in online repositories. They further suggest that a detailed step-by-step process in a Jupyter/IPython notebook or a ready-to-run Docker container should be provided to make reproduction attempts as easy as possible [@kim_2018]. The provision of a well documented Jupyter notebook would certainly be very useful in a reproduction attempt but version control is difficult in Jupyter notebook and it can be difficult to carry out extensive testing in that environment [@mueller_2018]. However, the notion of Docker containers is worth further investigation and leads us to workflow management systems. 

There are currently over 200 workflow management systems available, all with different pros and cons. Some popular choices include Nextflow, Snakemake, Galaxy and Toil. Generally, workflow management systems provide infrastructure to help users build and execute workflows and, in doing so, help to ensure reproducibility [@leipzig_2017]. They track execution provenance, allow parallel execution of tasks [@ditommaso_2017] and provide workflow diagrams to help the user visualise the order of the steps being executed in a particular workflow [@mallawaarachchi_2018]. Nextflow, probably the most popular choice of workflow management system [ref here], also uses Docker technology for multi-scale containerisation which, according to @ditommaso_2017, is necessary to ensure numerical stability of a workflow across different computational platforms. While the selection of a workflow management system is partly a personal preference, some systems offer advanced functionality that may be particularly useful. For example, both Galaxy and Toil also allow for multi-scale containerisation while Snakemake does not. Snakemake also doesn't offer cloud compatibility while Nextflow, Galaxy and Toil do. Both Toil and Galaxy offer Common Workflow Language compatibility ([@vivian_2017] and [@giardine_2005]) and Galaxy furthermore provides a Graphical User Interface which is particularly useful for researchers with little to no programming experience [@giardine_2005]. 

## Conclusion

Even when constrained to considering only the common sequencing platforms, there are major differences in read quality, error profiles, read lengths and cost. Illumina platforms generate high quality short reads at a very low cost while Nanopore reads are very long (up to 100kb) and low quality but also at a relatively low price when compared with Sanger sequencing. PacBio offers two main sequencing options; very long, low quality reads (similar to nanopore) or shorter (~15kbp), very high quality reads (Hifi reads). From just these four sequencing types, there are a huge number of different ways that a de novo assembly can be generated including both single source and hybrid methods. Of particular interest are assemblies from PacBio Hifi reads, ONT reads, PacBio long reads, ONT reads with Illumina polishing and PacBio long reads with Illumina Polishing. Popular tools for these assemblies include Canu, Nanopolish, **(more here based on rest of review)**. In assessing the quality of a de novo assembly, there is no real consensus as to the best method but measures of contig length and distribution, scaffold length and distribution, conserved gene content and basecall quality are common measures. Workflow reproducibility is an essential part of any bioinformatics project as, without it, results aren’t able to be confirmed. When findings aren’t able to be ratified by other researchers, scientific progress is hampered. By using a workflow management system in an aware and responsible manner, a bioinformatician can ensure that, at the very least, their workflows and analysis are reproducible. 


```{python, engine.path = '/home/chelsea/anaconda3/bin/python3'}
import sys
```


## References
