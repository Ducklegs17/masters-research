---
title: "Literature Review"
author: "Chelsea Matthews"
date: "27 August 2019"
output: html_document
bibliography: bibliography.bib
---

Aim: 
Explore strengths and weaknesses of main sequencing platforms. 
Work out a range of relatively common ways that people use this data to get a de-novo assembly.
Create a range of reproducible pipelines in snakemake or nextflow that use these methods for de-novo assembly. 
Benchmark these pipelines and the associated tools using another reproducible pipeline in snakemake or nextflow and a quality metric to show how long methods take and the quality of what you get out. 

## Introduction

Write intro. 

## First Generation

In 1953 the three-dimensional structure of DNA was solved by Watson and Crick [@watson_1953] but the next step, sequencing the order of nucleic acids, proved not to be trivial and it wasn't until 1977 that Sanger, Nickeln and Coulson published their gamechanging paper "DNA sequencing with chain-terminating inhibitors" [@sanger_1977]. While this wasn't the first sequencing technique developed, it was robust, easy to use and accurate and became the most common methodology for DNA sequencing in the following years [@heather_2016]. This method, commonly called Sanger sequencing, is based on the chain termination method. By using DNA Polymerase to synthesise the complementary strand of a single-stranded DNA molecule in the presence of a small number of chain-terminating dideoxynucleotides, double stranded fragments terminating at various different lengths are obtained. Through fractioning these fragments with electrophoresis, the resulting bands give the sequence of nucleotides [@sanger_1977; @frana_2002]. By 1991, much progress had been made in automating some of these steps and hence, scientists were projecting being able to sequence up to 1Mb per year in the very near future and, with further improvements in methodology and computing capability, up to 100Mb per year in the next 5 to 10 years [@hunkapiller_1991]. Sanger sequencing, **being 99.99%** accurate and producing reads of slightly less than 1kb in length [@heather_2016], was considered the gold standard for confirmation of variants in medical diagnoses for many years [beck_2016]. 

## Next Generation Techniques
The next generation of sequencing technology began to emerge around the same time that large-scale chain-termination methods were being developed [@heather_2016] and is characterised by the ability to sequence millions of DNA fragments at the same time [@behjati_2013]. Next Gen techniques can be subdivided into at least two sub-categories; second and third generation. Second generation techniques still in use include platforms such as Illumina (originally Solexa), Ion Torrent and SOLiD (Sequencing by Oglionucleotide Ligation and Detection). Of these, Illumina is by far the most commonly used platform [@greenleaf_2014] and provides a range of options for both benchtop and production scale sequencing producing both single and paired end reads of various lengths. Illumina uses a patented sequencing by synthesis technology where fluorescently tagged reversible terminators are imaged and cleaved as dNTPs are incorporated into a 'lawn' of amplified DNA fragments by DNA polymerase [@buermans_2014]. While this method results in very high quality reads (85% bp $\geq$ Q30 for 2 x 50bp versus 75% bp $\geq$ Q30 for 2 x 250bp on the NovaSeq 6000 System [@novaseq_nd]), the error distribution is not random [@schirmer_2016] and there are also limitations due to preferential amplification [@aird_2011; @krehenwinkel_2017] (though @pfeiffer_2018 found no evidence of bias due to amplificiation in NGS technologies in their study). 

There is some argument as to the defining difference between second and third generation sequencing technology but here it has been defined as the ability to sequence single molecules, thereby negating the need for amplification of DNA fragments and hence, removing the associated errors and biases [@schadt_2010]. PacBio provided the first widely used third generation technology using the SMRT (Single-Molecule, Real-Time) sequencing technique. In this technique, the real-time addition of bases is observed by immobilising DNA polymerase in a Zero Mode Waveguide illuminated from below by a laser. This enables the detection of the incorporation of individual phospholinked nucleotides against the bulk solution background (see Figure 1 below) [@eid_2009].  Using SMRT technology, there are two types of reads that can be generated; continuous long reads (CLRs) and circular consensus sequences (CCSs). CLRs were the first to become available and are characterised by very long read lengths (half of bases in reads >50kb on the Sequel II System with a 35kb size selected library) **reference pacbio website here** and an error rate of around 15% [@wei_2018]. Unlike first and second generation technologies, CLR errors are randomly distributed [@eid_2009; @pollard_2018] which means that, with enough depth, consensus across a number of reads can be reached [@pollard_2018]. 

<br>
<center>

![Figure 1: Principle of SMRT DNA sequencing. Source: [@eid_2009]](images/pacbioSMRT.png){width=50%}

</center>
<br>

PacBio has also recently introduced Hifi reads to the market [@wenger_2019]. Hifi reads are generated using Circular Consensus Sequencing (CSS) and boast sequences that are both long and accurate. The process is very similar to normal PacBio SMRT sequencing except that hairpin adapters are ligated to both ends of DNA fragments to allow the DNA polymerase to travel around a circular template molecule multiple times and hence, to generate a consensus sequence (see Figure 2 below) [@wenger_2019]. By selecting for fragments of approximately 15kb in length (using SageELF for separation into 3kb fractions and Agilent 2100 BioAnalyzer to identify fractions centered around 15kb), Sequel produced an average consensus sequence length of 13.5Â±1.2 kb with mean read concordance (when compared with the reference) at 99.8% (compared with 99.9% for illumina reads). This shows that PacBio CCS technology is capable of producing reads longer than Sanger sequencing techniques and of an equivalent quality to both Sanger and Illumina. 


<center>

![Figure 2: CCS generation. Source: [@pacificbiosciences_2019]](images/ccs-workflow.png){width=50%}

</center>
<br>

**Fix the pacbio github reference above. not sure how to reference without a doi**

Apart from PacBio, Oxford Nanopore is the other major provider of third generation sequencing technology. The MinION device, released in 2014 [@mikheyev_2014], costs just $1000, reports data in real time just two minutes after sequencing begins (with 10 minutes sample preparation time using the Rapid Kit) and is literally pocket sized **(Reference Nanopore website here)**. Nanopore technology directly sequences a single strand of DNA by binding the DNA template and complement with a hairpin adapter at one end and threading the resulting molecule through a nanopore using a molecular motor protein. As the molecule moves through the pore, changes in current are measured and a basecaller is used to determine the nucleotide sequence. The adapter sequences can then be removed from the resulting read and a consensus sequence generated (referred to as a 2D read) [@weirather_2017]. @weirather_2017 found that nanopore technology has a higher error rate than PacBio on a single pass (1D reads) (~20% vs ~14% respectively) but by using the 2D strategy, errors are reduced to approximately 13% (similar to a PacBio CLR). In particular, errors in calling homopolymers are common but it has been demonstrated that improvements to the basecalling algorithm can reduce these errors [@jain_2017]. 

## Pros and cons of using different data types for assembly

There are two methods of genome assembly; comparative and de novo. Comparative assembly utilises a reference genome of a closely related organism for guidance in the assembly process and is computationally cheap compared to de novo assembly which is usually done when there is no reference assembly of a related organism available [@kyriakidou_2018]. There are a massive number of projects currently being undertaken involving the de novo assembly of previously unsequenced organisms. Some of these include the Global Invertebrates Genomics Alliance (GIGA) looking to sequence 7,000 invertebrates [@gigacommunityofscientists_2014], the Genome 10K Project aiming to sequence 10,000 vertebrates [@koepfli_2015] and, most recently, the Earth BioGenome Project with the lofty goal of sequencing 'all of Earth's eukaryotic biodiversity over a period of ten years' [@lewin_2018] including a de novo assembly from each family. 

There are countless different ways to generate a de novo assembly but all of them begin with at least one type of sequencing data. In **???**, the human genome was sequenced with 10X coverage of Sanger sequencing **(reference here)** but the application of newer sequencing technologies to the same problem has resulted in great improvements to the assembly **(ref here)**.   


## Quality metrics for de novo assembly

With such a large number of sequencing projects underway, it is essential that we are able to accurately measure the quality and completeness of a de novo assembly [@yang_2019]. 
Probably the most commonly cited metric of assembly quality is contig N50 [@kyriakidou_2018] where a contig N50 of 1Mb implies that contigs of 1Mb and greater comprise 50% of the total assembly size. However, this only gives the reader an indication of the distribution of the contigs, can easily be manipulated through more or less stringent filtering of short contigs (ref here too https://www.molecularecologist.com/2017/04/the-first-problem-with-n50/) and can be artificially increased by specifying less stringent requirements for the concatenation of contigs [@gurevich_2013]. For these reasons, contig N50 alone isn't enough to measure assembly quality. QUAST, Quality Assessment Tool for genome assemblies released, improves on this by reporting contig Nx in addition to the total number of contigs, length of the largest contig, total length, GC content and the number of predicted genes in a de novo assembly [@gurevich_2013]. While this is inarguably better, it still doesn't give any information regarding base quality scores or the likely completeness of the assembly. 

Another approach to assessing assembly completeness is demonstrated by BUSCO (Benchmarking Universal Single-Copy Orthologs) which benchmarks the ortholog content of a new assembly against a curated ortholog set generated from a selection of organisms belonging to the same clade [@simo_2015; @waterhouse_2017]. BUSCO then reports the number of complete orthologs found with only a single copy, complete orthologs with multiple copies, fragmented orthologs and missing orthologs [@simo_2015]. The higher the number of complete single copy orthologs, the more complete the assembly is likely to be while high numbers of multiple copy orthologs are likely to indicate that the assembly of haplotypes is incorrect [@simo_2015]. Another method for assessing the completeness of an assembly (primarily in plants) is the LTR Assembly Index (LAI) [@ou_2018_lai]. Given that LRT Retrotransposons comprise a large portion of most plant genomes [@ou_2017] and that they are notoriously difficult to sequence due to their repetitive nature, a measure of assembly continuity of these regions is warranted [@ou_2018_lai]. Ou, Chen and Jiang noted that more complete genomes had a higher proportion of intract LTR elements when compared with less complete genomes (2018).To leverage this fact, they used LTR_retriever [@ou_2018_retriever] to first identify all LTR elements, then identified all intact LTR elements and finally applied a transformation to estimate LAI (see Equation 1 below) where LAI values less than 10 indicate draft genomes, between 10 and 20 are reference quality and LAI scores over 20 indicate gold standard assemblies [@ou_2018_lai]. 
By evaluating the assembly quality of of one of the more difficult regions to assemble, this method fills a gap in assessing plant assembly quality by allowing 
As a lot of new assemblies obtain very high BUSCO scores [@ou_2018_lai], this method helps to further differentiate between a good and a great assembly, if only in plants. 

<br>

$$LAI = \frac{Intact \space LTR \space element \space length}{Total \space LTR \space sequence \space length}\times 100 + 2.8138 \times (94 - Whole \space genome \space LTR \space identity)$$
<br>

And finally, it is worth investigating the methods proposed for assessing assembly quality in some of the major sequencing projects currently underway. The Earth BioGenome Project proposes to use a combination of contig N50, scaffold N50, a measure of chromosome assignment and a measure of basecall quality while avoiding assigning a direct classification (eg. gold, platinum, draft) [@lewin_2018]. GIGA proposes a similar approach but will also report, where possible, percent gaps, percent detection of conserved eukaryotic genes, a statistical assessment of assembly, alignment to any available syntenic or physical maps and mapping statistics of any availabla transcript data where possible [@gigacommunityofscientists_2014].   

DQA is an important task for de novo genome assembly and is especially useful today, as massive genome assemblies are in progress or available as drafts.

- SQUAT [@yang_2019] - SQUAT sequence quality assessment tool for de novo assemblies

[@liu_2012] - comparison of next generation sequencing systems

- comparison of long sequence assemblies

[@weirather_2017] - Comprehensive comparison of Pacific Biosciences and Oxford Nanopore Technologies and their applications to transcriptome analysis

[@giordano_2017] - De novo yeast genome assemblies from MinION, PacBio and MiSeq platforms

Pipelines for genome assemblies vary 

## Illumina pipeline and associated quality

## pacbio pipeline and quality


## nanopore pipeline and quality


- general pipeline for illumina/second gen) reads
- general pipeline for 3rd gen reads
- pipeline for fourth gen reads
- comparison of assembly with only illumina paired end reads vs longer poorer quality reads
- justify only looking at methods using long reads of some kind. 

## Gold Standard assembly algorithms review

-Illumina 
[@khan_2018] - study of de novo genome assemblers for paired end and single reads for illumina, uses prokaryotes and eukaryotes

## PacBio and Nanopore algorithms

- polishing?

 
-can hifi be used for polishing a nanopore assembly?

   
How much data will you get from a particular platform. 

What about quality trimming? How much do you lose?

Discuss algorithms (sorted by algorithm type) and refer back to the different types of sequencing. 

Problems with assembly? What can go wrong? How do the limitations of the sequencing platforms and assembly tools transfer into sequencing errors? 

Genome assembly quality (with and without a reference).
With a reference
-Quast
        
Without a reference. Explain why this is more difficult. 
- N50
- BUSCO score
- LTR Retrotransposons (particularly for plants?)




### Conclusion
Many authors have investigated the differences in genome quality and CPU hours between a range of different assembly tools/algorithms while others focus more on ???. Nowhere is there a single resource that informs a bioinformatician of the pros and cons at every decision along the way of generating a de-novo assembly which is supported by rigorous benchmarking and the provision of reproducible pipelines. 

******

# Outline

Introduction

Sequencing platforms available + strengths and weaknesses

De novo assembly and 

Quality assessment

Snakemake and Nextflow

*****

For example, python can be used. 

Review quality trimming tools

Review tools based on algorithms used. 

```{python, engine.path = '/home/chelsea/anaconda3/bin/python3'}
import sys
```


## References
