---
title: "Literature Review"
author: "Chelsea Matthews"
date: "27 August 2019"
output: html_document
bibliography: bibliography.bib
---

Aim: 
Explore strengths and weaknesses of main sequencing platforms. 
Work out a range of relatively common ways that people use this data to get a de-novo assembly.
Create a range of reproducible pipelines in snakemake or nextflow that use these methods for de-novo assembly. 
Benchmark these pipelines and the associated tools using another reproducible pipeline in snakemake or nextflow and a quality metric to show how long methods take and the quality of what you get out. 

## Introduction

Planning a de novo sequencing project is a matter of balancing costs, time, quality requirements and resources.  

There is a large amount of research comparing sequencing platforms on their ability to produce high quality assemblies and likewise, a lot of work has gone into comparing the capabilities of different algorithms for de novo assembly. This research often includes estimates of CPU hours for a particulr algorithm with a specific number of reads from a particular genome to generate the assembly but there doesn't seem to be a resource that helps a person designing an experiment decide where to spend their money. And it appears as though the amount of time spent doing the bioinformatics portion of an assembly can be quite extensive but is often not included in the pricing for an experiment. For example, should they spend a heap of money on Pacbio hifi reads and generate an assembly with very little effort in the bioinformatics department or should they spend less money, get a high coverage of Illumina data and scaffold it with the help of MinION or PacBIO CLR reads, get a similar quality and ? Will you get a 

A very important consideration when contemplating any sequencing project is how much the project will cost and it is important to realise that the cost of sequencing isn't always the most expensive part. 


@sovi_2016 ??

Write about the total cost of a pipeline and how this impacts funding. 

## First Generation

In 1953 the three-dimensional structure of DNA was solved by Watson and Crick [@watson_1953] but the next step, sequencing the order of nucleic acids, proved not to be trivial and it wasn't until 1977 that Sanger, Nickeln and Coulson published their gamechanging paper "DNA sequencing with chain-terminating inhibitors" [@sanger_1977]. While this wasn't the first sequencing technique developed, it was robust, easy to use and accurate and became the most common methodology for DNA sequencing in the following years [@heather_2016]. This method, commonly called Sanger sequencing, is based on the chain termination method. By using DNA Polymerase to synthesise the complementary strand of a single-stranded DNA molecule in the presence of a small number of chain-terminating dideoxynucleotides, double stranded fragments terminating at various different lengths are obtained. Through fractioning these fragments with electrophoresis, the resulting bands give the sequence of nucleotides [@sanger_1977; @frana_2002]. By 1991, much progress had been made in automating some of these steps and hence, scientists were projecting being able to sequence up to 1Mb per year in the very near future and, with further improvements in methodology and computing capability, up to 100Mb per year in the next 5 to 10 years [@hunkapiller_1991]. Sanger sequencing, **being 99.99%** accurate and producing reads of slightly less than 1kb in length [@heather_2016], was considered the gold standard for confirmation of variants in medical diagnoses for many years [beck_2016] despite comparatively low throughput and great expense.  

## Next Generation Techniques
The next generation of sequencing technology began to emerge around the same time that large-scale chain-termination methods were being developed [@heather_2016] and was characterised by the ability to sequence millions of DNA fragments at the same time [@behjati_2013]. Next Gen techniques can be subdivided into at least two sub-categories; second and third generation. Second generation techniques still in use include platforms such as Illumina (originally Solexa), Ion Torrent and SOLiD (Sequencing by Oglionucleotide Ligation and Detection). Of these, Illumina is by far the most commonly used platform [@greenleaf_2014] and provides a range of options for both benchtop and production scale sequencing producing both single and paired end reads of various lengths. Illumina uses a patented sequencing by synthesis technology where fluorescently tagged reversible terminators are imaged and cleaved as dNTPs are incorporated into a 'lawn' of amplified DNA fragments by DNA polymerase [@buermans_2014]. While this method results in very high quality reads (85% bp $\geq$ Q30 for 2 x 50bp versus 75% bp $\geq$ Q30 for 2 x 250bp on the NovaSeq 6000 System [@novaseq_nd]), the error distribution is not random [@schirmer_2016] and there are also limitations due to preferential amplification [@aird_2011; @krehenwinkel_2017] (though @pfeiffer_2018 found no evidence of bias due to amplificiation in NGS technologies in their study). 

There is some argument as to the defining difference between second and third generation sequencing technology but here it has been defined as the ability to sequence single molecules, thereby negating the need for amplification of DNA fragments and hence, removing the associated errors and biases [@schadt_2010]. PacBio provided the first widely used third generation technology using the SMRT (Single-Molecule, Real-Time) sequencing technique. In this technique, the real-time addition of bases is observed by immobilising a DNA polymerase in a Zero Mode Waveguide illuminated from below by a laser. This enables the detection of the incorporation of individual phospholinked nucleotides against the bulk solution background (see Figure 1 below) [@eid_2009]. PacBio sequencing also makes use of the SMRTbell template format where single stranded hairpin adapters are ligated to both ends of the double stranded fragment of DNA. Then, during sequencing using the SMRT platform, the DNA polymerase begins incorporating bases, starting at the primer, around the circular fragment. If it travels the full circle and reaches the location where it began, the DNA polymerase will displace the new strand and continue sequencing. Using this method, both the sense and anti-sense strands can be sequenced resulting in either very long reads of low quality (~15% error rate from a single pass raw read [@wei_2018]), or shorter more accurate reads (consensus sequences) can be generated from multiple passes of the molecule [@travers_2010] (see Figure 2). Unlike first and second generation technologies, PacBio errors are randomly distributed [@eid_2009] which means that consensus sequences greatly reduce basecalling error [@travers_2010] and that, in the case of very long fragments being sequenced, increasing coverage enables a consensus across reads to be reached (a method commonly used for polishing assemblies) [@chin_2013]. 

[@deamer_2016]

<br>
<center>

![Figure 1: Principle of SMRT DNA sequencing. Source: [@eid_2009]](images/pacbioSMRT.png){width=50%}


![Figure 2: CCS generation. Source: [@pacificbiosciences_2019]](images/ccs-workflow.png){width=50%}

</center>
<br>

**Fix the pacbio github reference above. not sure how to reference without a doi**

PacBio Hifi reads, an optimisation of circular consensus sequencing methods, [@wenger_2019] boast sequences that are both long and highly accurate. By selecting for fragments of approximately 15kb in length (using SageELF for separation into 3kb fractions and Agilent 2100 BioAnalyzer to identify fractions centered around 15kb), Sequel produced an average consensus sequence length of 13.5Â±1.2 kb with mean read concordance (when compared with the reference) at 99.8% (compared with 99.9% concordance for illumina reads). This demonstrates the utility of PacBio Hifi reads with sequence lengths much greater than Sanger or Illumina reads as well as comparable accuracy.  

Apart from PacBio, Oxford Nanopore is the other major provider of third generation sequencing technology. The MinION device was released in 2014 [@mikheyev_2014] and costs just $1000 US for a starter pack, reports data in real time beginning just two minutes after the sequencing run begins (with 10 minutes sample preparation time using the Rapid Kit) and is literally pocket sized **(Reference Nanopore website here)**. Nanopore technology directly sequences a single strand of DNA by threading it through a nanopore using a molecular motor protein. As the molecule moves through the pore, changes in current are measured and a basecaller is used to determine the nucleotide sequence based on these changes. Using this method, a 1D read is obtained as the fragment is sequenced in a single pass. To improve data quality, a 2D read can be generated by ligating a hairpin adapter to one end of a double strand of DNA. Then the fragment passes through the pore as a single strand and the same fragment is sequenced on both the sense and anti-sense strands [@weirather_2017]. @weirather_2017 found that nanopore technology has a higher error rate than PacBio for a single pass (1D reads) (~20% error rate for 1D reads vs ~14% error rate for PacBio raw reads) but by using the 2D strategy, errors are reduced to approximately 13% (similar to a PacBio raw reads). Unfortunately, due to the design of the nanopore platform, circularising the DNA fragments is not possible and so consensus sequences of more than two passes cannot be generated. 

## Quality metrics

There are two methods of genome assembly; comparative and de novo. Comparative assembly utilises a reference genome of a closely related organism for guidance in the assembly process and is computationally cheap compared to de novo assembly which is usually done when there is no reference assembly of a related organism available [@kyriakidou_2018]. There are a massive number of projects currently being undertaken involving the de novo assembly of previously unsequenced organisms. Some of these include the Global Invertebrates Genomics Alliance (GIGA) [@gigacommunityofscientists_2014], the Genome 10K Project [@koepfli_2015] and, most recently, the Earth BioGenome Project with the lofty goal of sequencing 'all of Earth's eukaryotic biodiversity over a period of ten years' [@lewin_2018] which includes a de novo assembly of at least one member from each family. 

With such a large number of sequencing projects underway [@gigacommunityofscientists_2014; @lewin_2018; @koepfli_2015], it is essential that we are able to accurately measure the quality and completeness of a de novo assembly [@yang_2019]. Probably the most commonly cited metric of assembly quality is contig N50 [@kyriakidou_2018] where a contig N50 of 1Mb implies that contigs of 1Mb and greater comprise 50% of the total assembly size. However, this only gives the reader an indication of the distribution of the contigs, can easily be manipulated through more or less stringent filtering of short contigs (ref here too https://www.molecularecologist.com/2017/04/the-first-problem-with-n50/) and can be artificially increased by specifying less stringent requirements for the concatenation of contigs [@gurevich_2013]. For these reasons, contig N50 alone isn't enough to measure assembly quality. QUAST, Quality Assessment Tool for genome assemblies released, improves on this by reporting contig Nx in addition to the total number of contigs, length of the largest contig, total length, GC content and the number of predicted genes in a de novo assembly [@gurevich_2013]. While this is inarguably better, it still doesn't give any information regarding base quality scores or the likely completeness of the assembly. 

Another approach to assessing assembly completeness is demonstrated by BUSCO (Benchmarking Universal Single-Copy Orthologs) which benchmarks the ortholog content of a new assembly against a curated ortholog set generated from a selection of organisms belonging to the same clade [@simo_2015; @waterhouse_2017]. BUSCO then reports the number of complete orthologs found with only a single copy, complete orthologs with multiple copies, fragmented orthologs and missing orthologs [@simo_2015]. The higher the number of complete single copy orthologs, the more complete the assembly is likely to be while high numbers of multiple copy orthologs are likely to indicate that the assembly of haplotypes is incorrect [@simo_2015]. Another method for assessing the completeness of an assembly (primarily in plants) is the LTR Assembly Index (LAI) [@ou_2018_lai]. Given that LRT Retrotransposons comprise a large portion of most plant genomes [@ou_2017] and that they are notoriously difficult to sequence due to their repetitive nature, a measure of assembly continuity of these regions is warranted [@ou_2018_lai]. Ou, Chen and Jiang noted that more complete genomes had a higher proportion of intract LTR elements when compared with less complete genomes (2018).To leverage this fact, they used LTR_retriever [@ou_2018_retriever] to first identify all LTR elements, then identified all intact LTR elements and finally applied a transformation to estimate LAI (see Equation 1 below) where LAI values less than 10 indicate draft genomes, between 10 and 20 are reference quality and LAI scores over 20 indicate gold standard assemblies [@ou_2018_lai]. 
By evaluating the assembly quality of of one of the more difficult regions to assemble, this method fills a gap in assessing plant assembly quality by allowing 
As a lot of new assemblies obtain very high BUSCO scores [@ou_2018_lai], this method helps to further differentiate between a good and a great assembly, if only in plants. 

<br>

$$LAI = \frac{Intact \space LTR \space element \space length}{Total \space LTR \space sequence \space length}\times 100 + 2.8138 \times (94 - Whole \space genome \space LTR \space identity)$$
<br>

And finally, it is worth investigating the methods proposed for assessing assembly quality in some of the major sequencing projects currently underway. The Earth BioGenome Project proposes to use a combination of contig N50, scaffold N50, a measure of chromosome assignment and a measure of basecall quality while avoiding assigning a direct classification (eg. gold, platinum, draft) [@lewin_2018]. GIGA proposes a similar approach but will also report, where possible, percent gaps, percent detection of conserved eukaryotic genes, a statistical assessment of assembly, alignment to any available syntenic or physical maps and mapping statistics of any availabla transcript data where possible [@gigacommunityofscientists_2014].   

## Pros and cons of using different data types for assembly

There are countless different paths to generating a de novo assembly but all of them begin with at least one type of sequencing data. In the beginning, the human genome was sequenced using primarily Sanger sequencing techniques [@lander_2001] but the advent of Next Generation techniques and the associated high throughput has made sequencing on a much larger scale possible [@pollard_2018]. Sequencing by synthesis methods meant that a entire human genome could be sequenced in a day compared with years for the first human genome draft using Sanger techniques [@behjati_2013]. However, Illumina reads (single or paired end), while very accurate, simply aren't long enough to reconstruct some of the more complex and repetitive regions of the genome [@pollard_2018; @sovi_2016], are not suited for discovery of structural variants [@cretustancu_2017]] and are further limited by preferential amplification of certain fragments. To overcome these shortfalls, third generation sequencing techniques have been invaluable, providing long reads to give context in extended repetitive regions, enabling the discovery of structural variants as well as removing bias associated with amplification of DNA fragments. A large amount of research has made it very clear that the incorporation of at least some long reads into a de novo assembly greatly improves the final assembly quality [@shin_2019; @jain_2018; @rhoads_2015].

## Discussion of different ways and tools to get a de novo assembly

The de novo assembly of a genome using long reads can be done in a number of ways and the complexity of these pipelines is somewhat dependent on the platform(s) selected for sequencing. For example, a simple pipeline for the assembly of a genome using PacBio Hifi reads would require the generation of consensus reads (ccs software) and the assembly of these reads into the final genome (peregrine) [@chin_2019]. In comparison, a hybrid assembly incorporating both nanopore and Illumina reads could require assembly of the nanopore reads (Canu), polishing contigs (racon), mapping the Illumina dataset to contigs (BWA mem) and error correction of contigs using the Illumina reads (pilon) [@nanoporetech_2016]. It should also be noted that hybrid assemblies can be completed in number of different ways, not limited to the following:  

- Short reads aligned to nanopore reads for error correction prior to assembly [@goodwin_2015]
- Nanopore reads used as scaffolds for the alignment of short reads [@ye_2016]
- Short reads used to polish errors from the nanopore assembly [@nanoporetech_2016]

It's interesting to note that the first method listed above was developed primarily to overcome the fact that existing assembly algorithms (Overlap-Layout-Consensus and DeBrujn graph) were unable to cope with long, low quality reads that were typical of third generation sequencing technology. Correcting errors in long reads at the beginning of a pipeline made it possible to use assembly algorithms originally designed for first generation sequencing data [@ye_2016]. 

There are many benchmarking studies comparing the performance of assembly and polishing algorithms on a particular dataset or a set of data. Recently, @wenger_2019 compared de novo human genome assemblies from PacBio CCS reads using Canu, FALCON and wtdbg2 and found that all tools had high contiguity (Canu N50 of 22.78Mb, FALCON N50 of 28.95Mb and wtdbg2 N50 of 15.43Mb) but that Canu and Falcon were over five times slower. However, Canu and FALCON were both able to span greater lengths of segmental duplications (63.6 and 46.1Mb respectivey) than wtdbg2 (26.4Mb) and Canu also managed to assemble some of the heterozygous alleles into separate contigs. They considered all three assemblies to be of high quality with wtdbg2 having the highest Phred concordance score of 44.6 with the reference (31.1 for Canu and 25.8 for FALCON). @giordano_2017 conducted a similar comparison of seven different long read assembly pipelines as well as comparing the differences between MinION and PacBio assemblies (at 31x coverage). Tools compared were Canu, FALCON, PBcR-Self, ABruijn, SMARTdenovo, Miniasm and Racon. They found that Miniasm was the fastest to run on both PacBio and ONT but also the least accurate which is unsurprising as Miniasm has no consensus step [@li_2016] and hence, the final assembly has an error rate the same as the raw reads. For the ONT data, Racon (which polishes a miniasm assembly) resulted in the highest identity at 98.5% while Canu and PBcR-Self were the most accurate for the PacBio dataset with identities of 99.92 and 99.93 respectively. Overall, @giordano_2017 suggest that SMARTdenovo and Canu (both based on the Overlap-Layout-Consensus algorithm) were the best performers with 'longest reference coverage, best or near best average identity, highest number of genes found and long Na50s' for both the PacBio and nanopore datasets. Interestingly, SMARTdenovo was also the fastest of all pipelines tested and took only 2 CPU hours while Canu took 15 hours for PacBio reads and 80 hours for ONT data. In comparison with these findings, @minei_2018 looked at the assembly quality of C. variabilis resulting from nanopore only assemblies (with 56x coverage of the ~46Mbp genome) using ABruijn, Miniasm and Canu and found that Canu resulted in just 26% coverage (2.5% identity) of the reference with ABruijn (the best performer) at 94.68% coverage (21% identity). They also reported that there was little improvement with coverage over 10-20X which is approximately in line with @giordano_2017 who found that an increase from 20X to 31X resulted in only slightly improved final accuracies. It is interesting that @giordano_2017 specifically noted Canu as being one of the best assemblers while @minei_2018 found it to be the worst. It is not clear what accounts for this although @minei_2018 suggest that Canu is simply not suitable for assembly of the *C. variabilis* genome using long reads. 

The pipelines discussed so far are all in relation to a single source of input data but hybrid approaches are common and can be very useful for managing costs while still obtaining a high quality assembly **(get a ref if possible)**. In addition to the pipelines above, @giordano_2017 also investigated a few hybrid pipelines using long reads for scaffolding a short read assembly. They tested npScarf, HybridSPAdes and SMIS, all with a SPAdes MiSeq assembly and found that npScarf resulted in the least contigs (21 for ONT and 22 for PacBio) and in the least processing time but that these contigs were heavily impacted by misassemblies. HybridSPAdes produced assemblies with the greatest reference coverage, the least mis-assemblies and the most genes but at the cost of CPU hours (for ONT, HybridSPAdes was 6 times slower than npScarf and 1.3 times slower than SMIS, for PacBio HybridSPAdes was 13 times slower than npScarf and 3 times slower than SMIS) [@giordano_2017]. Hybrid pipelines tested by @minei_2018 include SPAdes and MaSuRCA and they found that SPAdes resulted in an assembly with quality similar to a short read only assembly while MaSuRCA performed very well with the highest assembly quality and less contigs than the reference. 

write 300 more words on software.

## Reproducibility
Science depends on researchers being able to replicate the work of others but unfortunately, this isn't always possible. @baker found in a survey of 1500 scientists tht 70% of researchers had been unable to reproduce another scientist's experiment at some point while, even more worrying, 50% were unable to reproduce one of their own experiments. In bioinformatics, reproducibility refers specifically to being able to obtain the exact same results for an analysis using the original dataset and code but, even when the data and code are supplied, this isn't always possible [@kim_2018]. @kim_2018 suggest that the main barrier to reproducibility is the incompatibility among environments, software versions and different languages and that to combat this, researchers need to ensure their code is well written, well tested and made available in online repositories. They further suggest that a detailed step-by-step process in a Jupyter/IPython notebook or a ready-to-run Docker container should be provided to make reproduction attempts as easy as possible [@kim_2018]. The provision of a well documented Jupyter notebook would certainly be very useful in a reproduction attempt but version control is difficult in Jupyter notebook and can be difficult to carry out extensive testing on [@mueller_]. but the notion of Docker containers is perhaps even more useful. Nextflow, a workflow management system, uses Docker technology to containerise components of a pipeline while keeping track of execution provenance, allowing parallel execution of tasks and the adaption of existing pipelines written in any scripting language [@ditommaso_2017].  


but there are a number of limitations that should eb taken into account. For example, Jupyter notebooks doesn't enable simple code versioning, is difficult to test and due to the way you can jump between cells, can result in irreproducible experiments [@mueller_2018]. 
Other options pipeline frameworks provide another alternative to managing code and helping to ensure reproducibility [@leipzig_2017]. 


PacBio errors are random, ONT not so much. Many indels (@giordano_2017) 

1) Identify overlaps between reads
2) Layout reads and overlaps on a graph
3) Infer the consensus sequence [@li_2012]

In a comparison of de novo MinION and PacBio assemblies of yeast genomes @giordano2017 


Assembly of pacBio reads:
Canu
Falcon
wtdbg2 [@wenger_2019]
Peregrine [chen_2019]

Nanopore:
Canu
Racon
Falcon
SMARTdenovo 
PBcR-Self & Canu - best performers at low coverage [@giordano]



- Generation of consensus reads
- Assembly of consensus reads

Including:
- input data 
- coverage
- cpu hours
- tools
- polishing?

DQA is an important task for de novo genome assembly and is especially useful today, as massive genome assemblies are in progress or available as drafts.

- SQUAT [@yang_2019] - SQUAT sequence quality assessment tool for de novo assemblies

- general pipeline for illumina/second gen) reads
- general pipeline for 3rd gen reads
- pipeline for fourth gen reads
- comparison of assembly with only illumina paired end reads vs longer poorer quality reads
- justify only looking at methods using long reads of some kind. 

## Gold Standard assembly algorithms review


[@liu_2012] - comparison of next generation sequencing systems


There are numerous possible pipelines for a de novo assembly. Here we will consider and benchmark four different approaches:

- PacBio Hifi reads only (28x coverage with reads ~15kb long) using ??
- MinION reads only (52X coverage and 15x coverage by reads > 100kb) Racon + miniasm [@vaser_2017]
- MinION with Illumina polishing (illumina coverage 2 x 250bp (350bp gaps) at 40-50x coverage)
- MinION with PacBio Hifi reads for polishing


- pacbio hifi - comparison to all?. genome in 100 minutes. 

[@weirather_2017] - Comprehensive comparison of Pacific Biosciences and Oxford Nanopore Technologies and their applications to transcriptome analysis

[@giordano_2017] - De novo yeast genome assemblies from MinION, PacBio and MiSeq platforms

## Technology comparison for de novo assembly

- using both long and short reads (for polishing?) or consider polishing with pacbio hifi.

Illumina:
[@khan_2018] - study of de novo genome assemblers for paired end and single reads for illumina, uses prokaryotes and eukaryotes


Nanopore
It should also be noted that errors in calling bases in homopolymers are very common in nanopore reads but it has been demonstrated that improvements to the basecalling algorithm can reduce these errors [@jain_2018].

## PacBio and Nanopore algorithms

Discuss algorithms (sorted by algorithm type) and refer back to the different types of sequencing. 

Problems with assembly? What can go wrong? How do the limitations of the sequencing platforms and assembly tools transfer into sequencing errors? 

Genome assembly quality (with and without a reference).
With a reference
-Quast
        
Without a reference. Explain why this is more difficult. 
- N50
- BUSCO score
- LTR Retrotransposons (particularly for plants?)




### Conclusion
Many authors have investigated the differences in genome quality and CPU hours between a range of different assembly tools/algorithms while others focus more on ???. Nowhere is there a single resource that informs a bioinformatician of the pros and cons at every decision along the way of generating a de-novo assembly which is supported by rigorous benchmarking and the provision of reproducible pipelines. 

******

# Outline

Introduction

Sequencing platforms available + strengths and weaknesses

De novo assembly and 

Quality assessment

Snakemake and Nextflow

*****

For example, python can be used. 

Review quality trimming tools

Review tools based on algorithms used. 

```{python, engine.path = '/home/chelsea/anaconda3/bin/python3'}
import sys
```


## References
